
# Adversarial Attacks and Adversarial Training on CIFAR-10

## Overview

This repository contains an in-depth exploration of adversarial attacks and training methods in deep learning. Specifically, it focuses on using the **ResNet18** architecture to explore how adversarial examples can fool models trained on the **CIFAR-10 dataset**, and how adversarial training can help improve robustness against such attacks.

Adversarial attacks are a type of security vulnerability in machine learning models where small, deliberately crafted perturbations to the input can cause the model to make incorrect predictions with high confidence. These adversarial examples can drastically affect the model's performance, especially in critical systems like autonomous driving, security systems, and healthcare.

## What Are Adversarial Attacks?

Adversarial attacks are a well-known phenomenon in the field of deep learning. These attacks involve crafting inputs that are only slightly different from normal inputs but cause the model to misclassify the data. Such attacks can be performed on various types of models, including image classification networks, text-based models, and more.

For image classification, adversarial attacks typically involve adding carefully computed noise to images that deceive the model into predicting the wrong class. The two main types of adversarial attacks implemented in this project are:

- **FGSM (Fast Gradient Sign Method)**: This is a single-step attack that uses the gradients of the model to determine the direction in which to perturb the input image to maximize the loss function, causing the model to misclassify the image.
- **PGD (Projected Gradient Descent)**: This is a more advanced multi-step attack that iteratively perturbs the input image to maximize the loss over multiple iterations, creating stronger adversarial examples that are more difficult for the model to correctly classify.

### FGSM Attack (Fast Gradient Sign Method)

The **FGSM** attack is a simple but effective adversarial attack. It generates adversarial examples by adding a small perturbation to the input image in the direction of the gradient of the loss function. This attack is fast because it only requires computing the gradient of the loss with respect to the input image once. However, because it is a single-step method, the generated adversarial examples are often weaker than those produced by multi-step methods.

The formula for generating an adversarial example using FGSM is:
```math
x' = x + \epsilon \cdot sign(
abla_{x} J(	heta, x, y))
```
Where:
- `x` is the input image,
- `x'` is the adversarial example,
- `\epsilon` is a small scalar value representing the perturbation size,
- `
abla_{x} J(	heta, x, y)` is the gradient of the loss function with respect to the input image `x`,
- `sign()` is the sign function that determines the direction of the perturbation.

### PGD Attack (Projected Gradient Descent)

**PGD** is a more sophisticated attack compared to FGSM. PGD is an iterative attack that performs multiple steps of gradient ascent to generate adversarial examples. Each step adjusts the input slightly in the direction that maximizes the loss function, leading to more potent adversarial examples than those generated by FGSM.

The formula for PGD is similar to that of FGSM but involves iterating multiple times:
```math
x_{t+1} = clip(x_{t} + lpha \cdot sign(
abla_{x_t} J(	heta, x_t, y)), x - \epsilon, x + \epsilon)
```
Where:
- `x_t` is the input at step `t`,
- `lpha` is the step size,
- `clip()` ensures that the perturbations stay within a fixed range `[x - \epsilon, x + \epsilon]` to maintain a controlled level of perturbation.

PGD produces stronger adversarial examples, making it a more dangerous attack that is harder for models to defend against.

# Adversarial Training

## Adversarial Training: Making Models Robust

Adversarial training is a defense mechanism against adversarial attacks. The idea behind adversarial training is to improve the robustness of the model by including adversarial examples in the training data. By training the model on adversarially perturbed images, the model learns to be more resistant to adversarial attacks.

The basic workflow for adversarial training is as follows:

1. **Generate Adversarial Examples**: Use a method like FGSM or PGD to generate adversarial examples from the training set.
2. **Incorporate into Training Data**: Mix the adversarial examples with clean training data.
3. **Train the Model**: Train the model on this combined dataset, which includes both clean and adversarial examples.

By exposing the model to adversarial examples during training, the model learns to recognize the patterns in adversarial attacks, making it more resilient when faced with adversarial inputs during inference.

## Implementation Details

### ResNet18 Architecture on CIFAR-10

The model used in this project is a standard **ResNet18** architecture, which is a deep residual network with 18 layers. ResNet18 is popular for its ability to effectively train deep neural networks by using residual connections, which help mitigate the vanishing gradient problem.

The model is trained on the **CIFAR-10** dataset, which consists of 60,000 images across 10 different classes, including airplanes, cars, birds, and more. CIFAR-10 is a widely used dataset for image classification tasks, making it an ideal candidate for adversarial attack experiments.

### Attack Implementation

#### FGSM Attack with Torchattacks
The FGSM attack is implemented using the **torchattacks** library, which provides an easy-to-use interface for generating adversarial examples.

```python
import torchattacks

# Initialize FGSM attack
fgsm = torchattacks.FGSM(model, eps=0.1)

# Generate adversarial examples
adv_images = fgsm(images, labels)
```

#### PGD Attack from Scratch
The PGD attack is implemented from scratch, iterating through the input data to perform multiple gradient ascent steps.

```python
def pgd_attack(model, images, labels, epsilon, alpha, iters):
    images = images.clone().detach().to(device)
    original_images = images.clone().detach()
    
    for i in range(iters):
        images.requires_grad = True
        outputs = model(images)
        loss = criterion(outputs, labels)
        model.zero_grad()
        loss.backward()
        adv_images = images + alpha * images.grad.sign()
        eta = torch.clamp(adv_images - original_images, min=-epsilon, max=epsilon)
        images = torch.clamp(original_images + eta, min=0, max=1).detach()
    
    return images
```

### Adversarial Training Process

The adversarial training process follows these key steps:

1. **Train a Baseline Model**: Start by training a baseline ResNet18 model on clean CIFAR-10 data.
2. **Generate Adversarial Examples**: Use FGSM to create adversarial examples.
3. **Retrain the Model with Adversarial Data**: Incorporate adversarial examples into the training set and retrain the model to increase its robustness.

# Evaluation and Results

## Evaluating the Adversarially Trained Model

After completing the adversarial training, the model's performance and robustness are evaluated on both clean and adversarial examples. The following metrics are considered during evaluation:

- **Accuracy on Clean Data**: This is the accuracy of the model when tested on the clean (non-adversarial) CIFAR-10 test set.
- **Accuracy on Adversarial Data**: This measures the accuracy of the model when tested on adversarial examples generated using FGSM and PGD attacks.

The goal of adversarial training is to strike a balance between accuracy on clean data and robustness against adversarial examples. Ideally, the model should maintain high accuracy on clean data while significantly improving its accuracy on adversarially perturbed data.

### Results

The results section showcases the performance of the adversarially trained model in comparison to the baseline model. It includes tables or plots of accuracy metrics for clean data and adversarial examples. The adversarially trained model is expected to outperform the baseline model when it comes to handling adversarial attacks, demonstrating improved robustness.

For example:

| Model                 | Accuracy on Clean Data | Accuracy on Adversarial Data (FGSM) | Accuracy on Adversarial Data (PGD) |
|-----------------------|------------------------|-------------------------------------|------------------------------------|
| Baseline ResNet18      | 91%                    | 12%                                 | 5%                                |
| Adversarially Trained  | 89%                    | 64%                                 | 48%                               |

### Visualization

The notebook includes visualizations of the clean and adversarial examples, allowing you to compare the differences between the two. These visualizations demonstrate how small perturbations to an image can drastically alter the model's prediction.

### Future Work

Adversarial attacks and adversarial training are active areas of research. While this project covers two common attack methods (FGSM and PGD) and demonstrates the benefits of adversarial training, there is much more to explore. Future work could include:

1. **Exploring Other Attack Methods**: Experimenting with other adversarial attacks such as the **Carlini-Wagner attack**, **DeepFool**, or **AutoAttack** to further stress-test the model's robustness.
   
2. **Improving Adversarial Training**: Adversarial training can be further optimized by experimenting with different combinations of clean and adversarial data, or by introducing more sophisticated defense mechanisms like **ensemble adversarial training** or **distillation**.

3. **Evaluating on Other Datasets**: Extending adversarial training and evaluation to more complex datasets like **CIFAR-100** or **ImageNet** to assess the scalability of the approach.

4. **Robustness Beyond Adversarial Attacks**: Investigating model robustness against other forms of perturbations, such as **corruptions**, **noise**, or **distributional shifts**.

By continuing to research and refine adversarial defenses, the goal is to develop more resilient deep learning models that can maintain high performance even in the presence of malicious attacks or unexpected data shifts.

### Conclusion

Adversarial attacks pose a significant challenge to machine learning models, but with the right training techniques, it is possible to mitigate their effects. This project demonstrated the effectiveness of adversarial training in improving the robustness of a ResNet18 model on CIFAR-10. The results showed a clear improvement in the model's ability to handle adversarial examples, though there is still room for improvement.

### Contribution

Contributions are welcome! If you'd like to enhance this project, feel free to fork the repository and submit a pull request. Potential areas for contribution include exploring new adversarial attacks, improving adversarial training methods, and evaluating the model on other datasets.

### License

This project is licensed under the MIT License.
